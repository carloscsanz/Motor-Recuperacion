<html>

<head>
<meta http-equiv="Content-Language" content="en-us">
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<meta name="GENERATOR" content="Microsoft FrontPage 4.0">
<meta name="ProgId" content="FrontPage.Editor.Document">
<title>Asus Ti4200 128mb (V8420S)</title>
<base target="_blank">
</head>

<body>

<p align="center"><b><font face="Comic Sans MS" size="6">Asus Ti4200 128mb
(V8420S):</font></b></p>
<p align="center"><b><font face="Comic Sans MS" size="6">Notes and Comments</font></b></p>
<p align="center">&nbsp;</p>
<p align="center"><img border="0" src="AsusV8420S.jpg" width="640" height="480"></p>
<div align="center">
  <center>
  <table border="0" cellspacing="8" width="90%" style="font-family: Trebuchet MS; font-size: 12pt">
    <tr>
      <td width="100%" colspan="2"><font size="4"><b>Overview and Description</b></font></td>
    </tr>
    <tr>
      <td width="10%"></td>
    </center>
    <td width="50%">
      <p align="left">The Geforce4 Ti4200 is not a new card; it was introduced in May,
      2002.&nbsp;
      It occupies the lower rung of the Geforce4 product line (not to
      be confused with the Geforce4 <i> MX</i> series, which consists of revamped Geforce2s).&nbsp; The Ti4200
      comes in 64mb and 128mb versions.&nbsp; The Ti4400 and the Ti4600 were introduced around the same time.&nbsp; These cards
      primarily are distinguished from each other by their different graphic's
      chip and memory chip clock speeds.&nbsp; The Ti4200 reference design also
      calls for a physically
      smaller card (about 2 inches shorter in length), compared to the Ti4400
      and Ti4600.</p>
      <div align="center">
        <center>
        <table border="2" cellspacing="8" width="90%" style="font-family: Trebuchet MS; font-size: 12pt">
          <tr>
            <td width="33%" align="center"><b>Geforce4 Model</b></td>
            <td width="33%" align="center"><b>GPU Clock mHz</b></td>
            <td width="34%" align="center"><b>Memory Clock (Effective) mHz</b></td>
          </tr>
          <tr>
            <td width="33%" align="center">Ti4200 64mb</td>
            <td width="33%" align="center">250</td>
            <td width="34%" align="center">250 (500)</td>
          </tr>
          <tr>
            <td width="33%" align="center">Ti4200 128mb</td>
            <td width="33%" align="center">250</td>
            <td width="34%" align="center">222 (444)</td>
          </tr>
          <tr>
            <td width="33%" align="center" bgcolor="#00FFFF">Asus V8420S 128mb</td>
            <td width="33%" align="center" bgcolor="#00FFFF">260</td>
            <td width="34%" align="center" bgcolor="#00FFFF">275 (550)</td>
          </tr>
          <tr>
            <td width="33%" align="center">Ti4400 128mb</td>
            <td width="33%" align="center">275</td>
            <td width="34%" align="center">275 (550)</td>
          </tr>
          <tr>
            <td width="33%" align="center">Ti4600 128mb</td>
            <td width="33%" align="center">300</td>
            <td width="34%" align="center">325 (650)</td>
          </tr>
        </table>
        </center>
      </div>
      <p align="left">A quick look at the table above shows that Asus V8420S
      model Ti4200 is something of a Ti4200 Ultra.&nbsp; Its graphics processor
      is running 4% faster than the regular Ti4200s, and it's memory is as fast
      as that of the Ti4400s.&nbsp; The card also looks more like a Ti4400 than
      a Ti4200 in terms of its appearance.&nbsp; Most other Ti4200s use a
      smaller card format with a kind of memory chip that tends to be physically
      bigger.&nbsp; Not the V8420S.&nbsp; It basically is identical to the
      Ti4400 and Ti4600 boards in physical dimensions and appearance.&nbsp; (The
      <a href="http://www.digit-life.com/articles2/gf4/gf4ti4200-5.html">V8420S
      Review</a> done by the Digit-Life site shows this very well.&nbsp; Scroll
      down to where they compare pictures of the V8420S with the Nvidia
      reference model Ti4200 and Nvidia reference model Ti4600.)</p>
      <p align="left">Although the V8420S's memory is already running at the
      same speed as is typical for the faster Ti4400 cards, it is using memory
      rated at 3.3ns, which is faster than the typical 3.6ns memory used on most
      Ti4400s.&nbsp; The card that I'm looking at uses EtronTech memory, though
      other brands, such as Hynix, may also be found on these cards.</p>
      <p align="center"><img border="0" src="MemChip.jpg" width="366" height="272"></p>
      <p align="left">3.3ns translates into 300 mHz (or 600 mHz, DDR),
      suggesting that overclocking the memory on this card should not pose a
      problem.&nbsp; In fact, I've been running this card's memory
      conservatively overclocked to Ti4600 speeds (652 mHz DDR) for over a year
      and a half, now.&nbsp;&nbsp;</p>
      <p align="left">I've already situated the V8420S in relation to Nvidia'a
      Geforce4 lineup, but it might also be helpful to situate this card in
      relation to other Asus Ti4200 cards.&nbsp; They offer a V8420TD, which is
      their standard Ti4200 card (meaning a smaller card with the standard
      Ti4200 clock speeds) and the V8420 Deluxe, which is very similar to the
      V8420S but with the addition of Video In/Video Out (VIVO) and the 3D
      virtual reality (VR) glasses.&nbsp; The V8420S
      and the V8420 Deluxe share the same basic card layout and the same clock
      speeds.&nbsp; So, it might be best to think of the V8420S as a pared
      down Deluxe model, which retains the overclocked core and memory of the
      Deluxe model.</p>
      <p align="left">Asus also offers a couple of 8X AGP models.&nbsp; One is
      similar to the V8420S Deluxe described above, but the graphic core's
      speed has been bumped up to 275 mHz and the memory is running at 300 mHz
      (600 mHz DDR).&nbsp; This &quot;super fast&quot; Ti4200 8X AGP is the
      V9280S, and it seems more like a beefed up Ti4400
      than a Ti4200 card.&nbsp; There also is a more generic Ti4200 8X AGP,
      called the V9280TD, which runs at the typical Ti4200 core and
      memory speeds of 250 mHz and 256 mHz (512 mHz DDR), respectively. </p>
      <p align="left">As far as I can tell, whether you get a newer card capable
      of 8X AGP or not is unimportant.&nbsp; All the motherboards that support
      8X AGP also support 4X AGP, and I've yet to see a review that shows any
      advantage to having the 8X AGP card over a 4X AGP card. </p>
      <p align="left">Perhaps the most significant aspect of the Ti4200 that
      &quot;dates&quot; it is that it does not have native hardware support for
      the latest DirectX 9 graphics features.&nbsp; &nbsp;&nbsp; </p>
    </td>
  </tr>
  </table>
</div>
<p align="center">&nbsp;</p>

<div align="center">
  <center>
  <table border="0" cellspacing="8" width="90%" style="font-family: Trebuchet MS; font-size: 12pt">
    <tr>
      <td width="100%" colspan="2"><font size="4"><b>Performance</b></font></td>
    </tr>
    <tr>
      <td width="10%"></td>
      <td width="50%">I don't have an extensive &quot;lab&quot; for fully
        testing the capability of this video card and comparing it to others,
        but I would like to make a couple of comparisons to its predecessor, which it replaced.&nbsp;
        The system itself is underpowered
        as far as really being able to maximize the potential of a card like the Ti4200.&nbsp; It relies upon a PIII 1000 mHz processor,
        running on an Asus CUSL2 motherboard (i815 chipset) with 384 mb of
        SDRAM.&nbsp; The operating system is Win98SE with DirectX
        8.1.&nbsp; My previous card was also an Asus, a Geforce2 GTS 32mb card,
        the V7700 Pure.&nbsp; The following benchmarks for the Geforce2 card were
        run using Nvidia's 29.42 drivers.&nbsp; The benchmarks run with the Ti4200
        card were run using the newer 43.45D drivers from Asus.
        <p><b>Update January 31, 2004: </b>I had the opportunity to briefly test
        out this video card in a system running a Pentium 4 2.4 GHz
        processor.&nbsp; The results made very clear how much the Ti4200 is held
        back when it is paired with a PIII-1000 GHz processor.&nbsp; I didn't
        duplicate all my previously run benchmarks with the Pentium 4 system,
        but I added the overall 3DMark 2001SE score and the Vulpine GLMark
        score for the Ti4200 paired with a Pentium 4 2.4C to the appropriate
        graphs.</p>
        <p><b>Another Update January 26, 2005:</b> I added some information on
        how well this card overclocks.&nbsp; I've actually been running it at
        Ti4600 speeds (302 core/652 DDR memory), since soon after I installed
        this card.&nbsp; I've had no difficulty with these overclocked speeds
        for well over a year and a half, now.&nbsp; The results added below show
        how the card's performance scales going from it's original stock speed,
        through the equivalent of a Ti4600's speeds, and up to what appear to be
        the card's highest overclocked speeds.&nbsp; These overclocking results
        were run on an Athlon XP system.&nbsp; The CPU was running at 11 x 211
        mHz for a speed of 2300 mHz (maybe the equivalent of an Athlon XP 3300,
        if such a processor existed).</p>
        <p><b>3DMark 2001 SE</b></p>
        <p>I'll start out by looking at the results of this popular and easy to
        use benchmark.</p>
        <p align="center"><img border="0" src="3DMark2001SEoverall.jpg" width="640" height="480"></p>
      </center>
      <p align="left">These results give us two things to look at.&nbsp; We
      can see how the upgrading the video card from a Geforce2 to a Geforce4
      increases the performance of a Pentium 3 1000 GHz system, and we can see
      how we get nearly the same amount of improvement (almost 4000 points),
      when we upgrade the processor, as well, to a Pentium 4 2.4GHz model.</p>
      <p align="left"><b>Vulpine GLMark</b></p>
      <p align="left">This test is getting a bit dated, but it is still
      convenient.&nbsp; It provides an OpenGL test to compliment the DirectX
      capabilities of 3DMark2001.
      <p align="center"><img border="0" src="GLMark1024x32.jpg" width="640" height="480">
      <p align="left">Although still a healthy increase, the improvement in the
      score with the Ti4200 card, compared to the Geforce2, is not as
      significant as with the 3DMark tests, but when we add in the Pentium 4 2.4
      GHz processor, the score really takes off.&nbsp; Once again indicating
      that this TI4200 only going to be as fast as the CPU allows it to run.<p align="left"><b>Comanche
      4 Demo</b><p align="left">The Comanche 4 Demo includes a benchmarking
      utility, so we'll turn to it next.&nbsp; It provides more of a real
      (game) world test than the above benchmarks do.<p align="center"><img border="0" src="Commanche4_800x32.jpg" width="640" height="480"><p align="left">Here
      we see the difference between the two video cards narrowing.&nbsp; In
      part, this may be because the demo is fairly dependent upon the CPU
      for performance, as well as the video card.&nbsp;&nbsp;<p align="left">This finding provides a nice lead into the
      next benchmark, which is based on Battlezone 2.&nbsp; Although Battlezone
      2 is visually very demanding, having a powerful CPU is
      also critical to your system keeping up with this game.<p align="left"><b>Battlezone
      2</b><p align="center"><img border="0" src="Sheep.jpg" width="695" height="262"><p align="center"><img border="0" src="Battlezone2_1024x32.jpg" width="640" height="480"><p align="left">The
      average frame rate was taken from a campaign mission in the <a href="http://www.bz2cp.com/"> Forgotten Enemies</a> mod
      of Battlezone 2.&nbsp; During this
      time, your units come under attack from a large enemy force; so, there is plenty of activity
      going on.&nbsp;&nbsp;<p align="left">I
      think the relatively small difference made by the newer video card is due
      to how much the CPU is critical to running this game.&nbsp; My own
      speculation is that this is due to a couple of factors.&nbsp; First, there
      are a lot of AI units to
      control and manage the pathing for in this game, and second, the game engine is a bit old
      and can't take advantage of what newer video cards can do with visual
      effects &quot;in hardware&quot;.&nbsp; Instead, it would appear that the CPU is
      being left to carry a significant part of the visual rendering load
      (especially when it comes to lighting effects).&nbsp; This would have been
      a good benchmark to retest on the Pentium 4 2.4 GHz system, but
      unfortunately I didn't do so.<p align="left"><b>Overclocking Capabilities
      and Results</b><p align="left">I've had good results with overclocking
      this video card; I've basically been using it as a Ti4600 with the core at
      302 mHz and the memory at 652 mHz DDR for over a year and a half.&nbsp; I
      should mention that I replaced the stock heatsink and fan with a
      Thermaltake replacement kit.&nbsp; This was because the stock fan soon
      developed an annoying sound when it was starting up.&nbsp; As it turned
      out, the fan with the Thermaltake heatsink was not any better and soon
      started making noises, itself.&nbsp; I've since
      replaced the Thermaltake fan with one from a Radeon 9700 Pro.<p align="left">The biggest problem with the Thermaltake heatsink was that it
      would not install properly, because the bottom was not flat.&nbsp; It had
      such a hump to it that the heatsink would rock to one side and lift up the
      other half of it from contact with the graphics chip.&nbsp;
      After taking the time to lap the bottom of the heatsink,
      it now fits flush with the graphics chip, and this all copper
      heatsink performs quite well.<p align="center"><img border="0" src="ThermaltakeHS.jpg" width="454" height="375"><p align="left">The
      Thermaltake heatsink kit also comes with some little heatsinks for the
      memory chips.&nbsp; I've only gotten around to installing them lately.&nbsp; The kit includes some double-sided, sticky, thermal tape
      for attaching these heatsinks, but I opted to permanently glue them
      on with Arctic Silver's thermal epoxy.&nbsp; I can't say that these memory
      heatsinks have made much difference in terms of how far I can overclock
      the memory.&nbsp; I don't think the memory chips&nbsp; were ever getting
      that hot to begin with.&nbsp; Nevertheless, they seem to be in good
      contact with the memory chips and doing whatever they can to add some
      further cooling.&nbsp; This conclusion is based on my careful assessment,
      which involves placing a finger to the heatsinks while the computer is busy
      working its way through video benchmarks.<p align="center"><img border="0" src="CardInstallFront.jpg" width="507" height="402"><p align="left">For
      some reason, the heatsinks were of very different heights.&nbsp; I put the
      taller heatsinks on the front and the shallower ones on the back.<p align="center"><img border="0" src="CardInstallBack.jpg" width="267" height="349"><p align="left">I
      selected three sets of core and memory speeds to show how this card scales
      when being overclocked.&nbsp; The 261 mHz core and the 550 mHz DDR memory
      speeds are the stock settings.&nbsp; The 302 mHz/652 mHz DDR speeds are
      the same as those used by a stock Ti4600.&nbsp;&nbsp; And, the 339 mHz/689
      mHz DDR speeds appear to be close to the maximums that this card can run.&nbsp; The card locked up almost immediately when
      trying to run a 3D benchmark with the core at 350 mHz, and I noticed
      some slight artifacts appearing with the card's memory running at 697 mHz
      DDR.<p align="left">First, let's look at how things scale running
      3DMark2001SE at the default resolution of 1024 x 768 x 32 bit color.<p align="center"><img border="0" src="1024_3DMark_OC.jpg" width="640" height="480"><p align="left">That's
      not too bad.&nbsp; We've bumped the core up by nearly 30% and the memory
      up by about 25%, and as a result we are seeing about a 15% increase in
      performance.<p align="left">We see even better results when we look at
      3DMark2001SE run at a high resolution of 1600 x1200 x 32 bit color and
      when we look at Aquamark3 run at its default 1024x 768 x 32 settings.<p align="center">&nbsp;<img border="0" src="1600_3DMark_OC.jpg" width="640" height="480"><p align="center"><img border="0" src="1024_Aquamark3_OC.jpg" width="640" height="480"><p align="left">We are now getting a 22% increase in performance on
      3DMark2001SE, and we see a 28% increase in performance on the Aquamark3
      benchmark.&nbsp; Given these results, I think we can safely say that this
      video card overclocks very well.<p align="left"><b>Battlefield
      1942</b><p align="left">This game was my primary motivation
      for upgrading my system's video card.&nbsp; Battlefield 1942 just wasn't
      happy on a 32 mb video card.&nbsp; 32 mb of video memory meets the minimum
      specifications for a system to play the game, but the recommendation is
      for 64 mb of video memory.&nbsp; The game is quite playable using
      the 32 mb Geforce 2 video card, but the graphics details and the texture
      sizes have to be kept to a minimum.&nbsp; Bumping the graphics details up beyond their lowest settings results in the game becoming unplayable due
      to a jerking or &quot;chugging&quot; quality to the video rendering.&nbsp; Instead of
      a smooth fluid flow of scenery, as you panned your view around, you would get a constant series of hesitations or
      pauses.&nbsp; My hunch is that the video card was having
      to access system memory because it had used up all the room available to the
      onboard video memory.&nbsp; This is a much slower process than if
      everything can be handled on the video card, and the game had to wait on
      the slower data source.<p align="left">Since the problem that I ran into
      with this game wasn't really related to frames per second but rather to
      the level of video quality that I could use, I think some pictures will tell the story better than
      numbers and graphs can.&nbsp; In the following series of paired pictures, the
      first is taken at 800x600x16 resolution with all the graphics details
      turned down (except for view distance), and the
      second is a similar picture taken at 800x600x32 resolution with all
      the graphics details set to their highest.&nbsp; All these pictures were
      taken using the Ti4200 card, but they convey what the difference was like
      in going from the Geforce2 card to the Ti4200 card.<p align="center"><img border="0" src="LowQual1.jpg" width="513" height="480"><p align="center"><img border="0" src="HiQual1.jpg" width="549" height="429"><p align="center"><img border="0" src="LowQual2.jpg" width="483" height="421"><p align="center"><img border="0" src="HiQual2.jpg" width="490" height="418"><p align="center"><img border="0" src="LowQual3.jpg" width="473" height="276"><p align="center"><img border="0" src="HiQual3.jpg" width="600" height="411"><p align="left">Unfortunately,
      these snapshots don't fully convey the impact of being able to play this
      game with the higher graphics settings enabled, because they don't carry
      the feeling of being immersed in the game world.&nbsp; Also, since I had
      gotten use to playing the game at the lower graphics settings, the change to something
      much sharper and more vivid was startling.&nbsp; I had the sense of finally
      getting to see the game as it was suppose to look.&nbsp; It was as if I
      had been working in a dimly lit room and suddenly someone turned on the
      lights. </td>
  </tr>
  </table>
</div>

<p align="left">&nbsp;</p>
<div align="center">
  <center>
  <table border="0" cellspacing="8" width="90%" style="font-family: Trebuchet MS; font-size: 12pt">
    <tr>
      <td width="100%" colspan="2"><font size="4"><b>Conclusions</b></font></td>
    </tr>
    <tr>
      <td width="10%"></td>
      <td width="50%">With this latest update to this review, I think that I can
        confidently say that the V8420S Ti4200 video card has been a good addition to my system.&nbsp; It provides significantly more rendering
        power than my Geforce2 did, and the extra video memory
        allows me to keep up with the trend in PC game development to use more video memory for texture storage.&nbsp; The
        benefit of this can easily be seen with the popular game, Battlefield
        1942.<p>The Geforce 4 video cards are no longer really available (except
        perhaps on eBay), as Nvidia is a couple of generations beyond their
        Ti4000 cards by now.&nbsp; However, this card continues to perform well with my favorite games,
        since they do not require DirectX 9.&nbsp; I suspect that my next video
        card upgrade will come when there finally is a killer online game that does make use of
        DirectX 9 features.</p>
        <p>In the meanwhile, I've definitely gotten my money's worth out of this
        $150 card.&nbsp; My only complaints about it have been minor ones.&nbsp;&nbsp;</p>
        <p> The
        fan that was included, like those on many video cards these days, turned
        out to be a poor one.&nbsp;&nbsp;</p>
        <p> I also wish that Asus had made available
        hardware monitoring for this video card.&nbsp; The Asus V8420
        Deluxe (as well as my old Geforce2 GTS, for that matter) has hardware monitoring
        capabilities built into the board, and Asus has a nice hardware
        monitoring utility, SmartDoctor, for keeping an eye on things like the
        video card's fan speed, the graphics chip's temperature, and video
        card's voltages.&nbsp; Given how this card is billed as part of Asus's
        &quot;Super Fast&quot; line of video cards, it seems like the hardware
        monitoring should have been kept, even when other items were stripped away
        from the Deluxe package to make this model.&nbsp; The hardware
        monitor would have been useful to the enthusiast as he or she was tweaking this
        card's clock setting or perhaps trying out an after market cooling
        solution.&nbsp; By omitting the hardware monitoring on the V8420S, I think that Asus missed an opportunity to make
        it even more appealing to the portion of the market that they appeared to be
        pitching this &quot;overclocked&quot;, no frills, video card to.&nbsp;&nbsp;</p>
 </td>
    </tr>
  </table>
  </center>
</div>
<p>&nbsp;</p>
<div align="center">
  <center>
  <table border="0" cellspacing="8" width="90%" style="font-family: Trebuchet MS; font-size: 12pt">
    <tr>
      <td width="100%" colspan="2"><font size="4"><b>More Reading</b></font></td>
    </tr>
    <tr>
      <td width="10%"></td>
      <td width="50%">The official <a href="http://usa.asus.com/products/vga/v8420std/overview.htm"> Asus blurb on the
        V8420S</a>.
        <p>The <a href="http://www.digit-life.com/articles2/gf4/gf4ti4200-5.html">Digit-Life
        review of the V8420S</a>, mentioned earlier.</p>
        <p>Since there aren't many reviews of the V8420S, I've thrown in some
        looks at the Deluxe model, which the V8420S seems to be derived from</p>
        <p><a href="http://www.hardocp.com/article.html?art=MzMxLDE=">HardOCP
        review of the V8420 Deluxe</a>.</p>
        <p><a href="http://www.hothardware.com/hh_files/S&amp;V/asus_ti_4200d.shtml">HotHardware
        review of the V8420 Deluxe</a>.</p>
        <p><a href="http://www.8ballshardware.com/articles/v8420/page1.cfm">8
        Ball's Hardware review of the V8420 Deluxe</a>.</p>
        <p><a href="http://www.gotapex.com/reviews.php?rev=videocards/8420D/index.html">Got
        Apex review of the V8420 Deluxe</a>.</td>
    </tr>
  </table>
  </center>
</div>

<p>&nbsp;</p>
<p align="left"><font face="Comic Sans MS" size="2">Original review June 7, 2003</font></p>
<p align="left"><font face="Comic Sans MS" size="2">Last revised January 29,
2005</font></p>
<p align="left"><b><font face="Comic Sans MS" size="3">Back to <a href="../../index.htm">Sequoyah
Computer</a></font></b></p>

</body>

</html>
